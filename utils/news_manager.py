# -*- coding:utf-8 -*-
import aiohttp
import datetime
import json
from typing import List, Dict
from urllib.parse import quote

class NewsManager:
    def __init__(self):
        # ãƒ†ãƒƒã‚¯ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.tech_keywords = [
            "AI", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ChatGPT", "ç”ŸæˆAI",
            "DX", "ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©", "ã‚¯ãƒ©ã‚¦ãƒ‰", "AWS", "Azure",
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "Python", "JavaScript", "React",
            "ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—", "èµ·æ¥­", "VC", "æŠ•è³‡",
            "ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³", "Web3", "NFT", "æš—å·é€šè²¨",
            "IoT", "5G", "AR", "VR", "ãƒ¡ã‚¿ãƒãƒ¼ã‚¹",
            "ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹"
        ]
        
        # Googleãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã®ãƒ™ãƒ¼ã‚¹URL
        self.google_news_base = "https://news.google.com/rss/search"
    
    async def get_tech_news(self, max_articles: int = 5) -> List[Dict]:
        """ãƒ†ãƒƒã‚¯ç³»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        all_articles = []
        
        # è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã¦å¤šæ§˜æ€§ç¢ºä¿
        for keyword in self.tech_keywords[:3]:  # ä¸Šä½3ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            try:
                articles = await self._fetch_google_news(keyword, max_articles=2)
                all_articles.extend(articles)
            except Exception as e:
                print(f"Error fetching news for {keyword}: {e}")
                continue
        
        # é‡è¤‡é™¤å»ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        unique_articles = {}
        for article in all_articles:
            title = article.get('title', '')
            if title and title not in unique_articles:
                unique_articles[title] = article
        
        # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_articles = sorted(
            unique_articles.values(),
            key=lambda x: x.get('pub_date', ''),
            reverse=True
        )
        
        return sorted_articles[:max_articles]
    
    async def _fetch_google_news(self, keyword: str, max_articles: int = 5) -> List[Dict]:
        """Googleãƒ‹ãƒ¥ãƒ¼ã‚¹RSSã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
        query = f"{keyword} ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ OR ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³"
        encoded_query = quote(query)
        url = f"{self.google_news_base}?q={encoded_query}&hl=ja&gl=JP&ceid=JP:ja"
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    timeout=aiohttp.ClientTimeout(total=10),
                    headers={'User-Agent': 'Mozilla/5.0 (compatible; DJEyes-Bot/1.0)'}
                ) as response:
                    if response.status == 200:
                        content = await response.text()
                        return self._parse_rss_content(content, max_articles)
                    else:
                        print(f"Google News API error: {response.status}")
                        return []
        except Exception as e:
            print(f"Error fetching Google News: {e}")
            return []
    
    def _parse_rss_content(self, rss_content: str, max_articles: int) -> List[Dict]:
        """RSS XMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ‘ãƒ¼ã‚¹"""
        import xml.etree.ElementTree as ET
        
        articles = []
        try:
            root = ET.fromstring(rss_content)
            
            # RSS 2.0å½¢å¼ã®itemè¦ç´ ã‚’æ¢ç´¢
            items = root.findall('.//item')[:max_articles]
            
            for item in items:
                title_elem = item.find('title')
                link_elem = item.find('link')
                pub_date_elem = item.find('pubDate')
                description_elem = item.find('description')
                
                if title_elem is not None and link_elem is not None:
                    article = {
                        'title': title_elem.text or '',
                        'url': link_elem.text or '',
                        'pub_date': pub_date_elem.text if pub_date_elem is not None else '',
                        'description': description_elem.text if description_elem is not None else ''
                    }
                    
                    # HTMLã‚¿ã‚°ã‚’é™¤å»
                    article['description'] = self._clean_html(article['description'])
                    
                    articles.append(article)
                    
        except ET.ParseError as e:
            print(f"RSS parsing error: {e}")
        except Exception as e:
            print(f"Error parsing RSS: {e}")
            
        return articles
    
    def _clean_html(self, text: str) -> str:
        """HTMLã‚¿ã‚°ã‚’é™¤å»"""
        import re
        if not text:
            return ""
        
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        clean_text = re.sub(r'<[^>]+>', '', text)
        # ä½™åˆ†ãªç©ºç™½ã‚’æ•´ç†
        clean_text = ' '.join(clean_text.split())
        
        return clean_text[:200] + "..." if len(clean_text) > 200 else clean_text
    
    def get_fallback_tech_news(self) -> List[Dict]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹"""
        today = datetime.datetime.now()
        
        return [
            {
                'title': 'ç”ŸæˆAIå¸‚å ´ãŒæ€¥æ‹¡å¤§ã€ä¼æ¥­ã®å°å…¥ç‡70%çªç ´',
                'url': 'https://example.com/ai-market-growth',
                'pub_date': today.strftime('%Y-%m-%d'),
                'description': 'æœ€æ–°èª¿æŸ»ã«ã‚ˆã‚Šã€ç”ŸæˆAIæŠ€è¡“ã®ä¼æ¥­å°å…¥ãŒåŠ é€Ÿã—ã¦ã„ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸã€‚'
            },
            {
                'title': 'ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–æŠ€è¡“ã®æ¡ç”¨ãŒåŠ é€Ÿ',
                'url': 'https://example.com/cloud-native-adoption',
                'pub_date': today.strftime('%Y-%m-%d'),
                'description': 'ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã¨ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹æŠ€è¡“ã®å°å…¥ä¼æ¥­ãŒå‰å¹´æ¯”150%å¢—åŠ ã€‚'
            },
            {
                'title': 'ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æŠ•è³‡ãŒéå»æœ€é«˜ã«',
                'url': 'https://example.com/cybersecurity-investment',
                'pub_date': today.strftime('%Y-%m-%d'),
                'description': 'DXæ¨é€²ã«ä¼´ã„ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã¸ã®æŠ•è³‡ãŒæ€¥å¢—ã—ã¦ã„ã‚‹ã€‚'
            }
        ]
    
    def format_news_for_embed(self, articles: List[Dict]) -> Dict:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’Embedç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not articles:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨˜äº‹ã‚’ä½¿ç”¨
            articles = self.get_fallback_tech_news()
        
        # è¨˜äº‹ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆDiscord 1024æ–‡å­—åˆ¶é™å¯¾å¿œï¼‰
        article_list = []
        total_length = 0
        max_field_length = 900  # 1024æ–‡å­—åˆ¶é™ã«ä½™è£•ã‚’æŒãŸã›ã‚‹
        
        for i, article in enumerate(articles[:3], 1):  # æœ€å¤§3è¨˜äº‹ã«åˆ¶é™
            title = article.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')
            url = article.get('url', '')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’çŸ­ç¸®ï¼ˆ25æ–‡å­—åˆ¶é™ï¼‰
            if len(title) > 25:
                title = title[:22] + "..."
            
            if url and url != 'https://example.com':
                article_info = f"**{title}**\nğŸ”— [è©³ç´°]({url})"
            else:
                article_info = f"**{title}**"
            
            # æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
            if total_length + len(article_info) + 2 > max_field_length:  # +2 for \n\n
                break
                
            article_list.append(article_info)
            total_length += len(article_info) + 2
        
        return {
            "title": "ğŸŒ æœ€æ–°ãƒ†ãƒƒã‚¯ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±",
            "description": "æœ€æ–°ã®ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼å‹•å‘ã‚’ãŠå±Šã‘ã—ã¾ã™ï¼",
            "fields": [
                {
                    "name": "ğŸ“° æ³¨ç›®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                    "value": "\n\n".join(article_list) if article_list else "ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "inline": False
                },
                {
                    "name": "ğŸ’¡ ãƒ“ã‚¸ãƒã‚¹ã¸ã®æ´»ç”¨",
                    "value": "â€¢ æœ€æ–°æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã®æŠŠæ¡\nâ€¢ ç«¶åˆåˆ†æã¨æˆ¦ç•¥ç«‹æ¡ˆ\nâ€¢ æ–°ã—ã„ãƒ“ã‚¸ãƒã‚¹æ©Ÿä¼šã®ç™ºè¦‹",
                    "inline": False
                }
            ],
            "color": 0x1DA1F2  # Twitter blue
        }