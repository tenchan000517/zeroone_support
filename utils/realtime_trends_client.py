# -*- coding:utf-8 -*-
"""
Realtime Trends Client
Node.jsç‰ˆã®é«˜å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ã‚·ã‚¹ãƒ†ãƒ ã‚’Pythonã«ç§»æ¤

ã‚ªãƒªã‚¸ãƒŠãƒ«: find-to-do-site/src/lib/realtime-trends.ts
ç§»æ¤è€…: Claude Code

æ©Ÿèƒ½:
- Zenn API: æŠ€è¡“è¨˜äº‹ï¼ˆã„ã„ã­æ•°é †ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã€æœ€æ–°ï¼‰
- Hacker News: æµ·å¤–æŠ€è¡“æƒ…å ±
- GitHub Trending: äººæ°—ãƒªãƒã‚¸ãƒˆãƒª  
- Google News RSS: æ—¥æœ¬èªæŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹
- å„åˆ†é‡å°‚é–€è¨˜äº‹å–å¾—
- è‡ªå‹•ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
- é‡è¤‡é™¤å»ã‚·ã‚¹ãƒ†ãƒ 
"""

import aiohttp
import asyncio
import json
import xml.etree.ElementTree as ET
import re
from typing import List, Dict, Optional, Set
from datetime import datetime, timedelta
import random
from dataclasses import dataclass
from urllib.parse import quote

@dataclass
class TrendItem:
    """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¢ã‚¤ãƒ†ãƒ ã®çµ±ä¸€ãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    id: str
    title: str
    url: str
    score: int = 0
    likes: int = 0
    comments: int = 0
    source: str = ""
    published_at: str = ""
    topics: List[str] = None
    category: str = ""
    description: str = ""
    
    def __post_init__(self):
        if self.topics is None:
            self.topics = []

class RealtimeTrendsClient:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.seen_urls: Set[str] = set()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ï¼ˆNode.jsç‰ˆã‹ã‚‰ç§»æ¤ï¼‰
        self.category_keywords = {
            'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°': [
                'programming', 'code', 'coding', 'algorithm', 'data structure', 
                'clean code', 'refactoring', 'software engineering', 'design pattern',
                'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'ãƒ‡ãƒ¼ã‚¿æ§‹é€ ', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦',
                'python', 'rust', 'go', 'java', 'c++', 'c#', 'kotlin', 'swift',
                'ruby', 'php', 'scala', 'dart',
                'node.js', 'express', 'fastify', 'nest.js', 'spring', 'django',
                'flask', 'laravel', 'rails'
            ],
            'ã‚¦ã‚§ãƒ–é–‹ç™º': [
                'javascript', 'typescript', 'es6', 'es2024', 'vanilla js',
                'frontend', 'web development', 'html', 'css', 'sass', 'tailwind', 'bootstrap',
                'ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰', 'ã‚¦ã‚§ãƒ–é–‹ç™º', 'ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é–‹ç™º', 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆåˆ¶ä½œ',
                'react', 'vue', 'angular', 'svelte', 'next.js', 'nuxt', 'gatsby',
                'react native', 'vue3', 'react hooks', 'vue composition',
                'css grid', 'flexbox', 'css modules', 'styled components',
                'responsive', 'mobile-first', 'ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–', 'ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³',
                'pwa', 'spa', 'ssr', 'ssg', 'jamstack',
                'web components', 'web assembly', 'service worker', 'websocket'
            ],
            'ç”ŸæˆAI': [
                'AI', 'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯',
                'artificial intelligence', 'machine learning', 'deep learning',
                'neural network', 'transformer', 'gpt', 'llm', 'claude', 'chatgpt',
                'openai', 'anthropic', 'gemini', 'bard', 'copilot',
                'ç”ŸæˆAI', 'generative ai', 'prompt engineering', 'fine-tuning',
                'rag', 'retrieval augmented generation', 'llama', 'mistral'
            ],
            'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»AIé–‹ç™º': [
                'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹', 'ãƒ‡ãƒ¼ã‚¿åˆ†æ', 'python', 'pandas', 'çµ±è¨ˆ', 'å¯è¦–åŒ–', 'äºˆæ¸¬',
                'data science', 'data analysis', 'statistics', 'visualization',
                'big data', 'ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿', 'ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°', 'analytics',
                'jupyter', 'notebook', 'matplotlib', 'seaborn', 'plotly',
                'tensorflow', 'pytorch', 'scikit-learn', 'numpy', 'scipy'
            ],
            'ã‚­ãƒ£ãƒªã‚¢': [
                'ã‚­ãƒ£ãƒªã‚¢', 'è»¢è·', 'ã‚¹ã‚­ãƒ«', 'å¹´å', 'é¢æ¥', 'å±¥æ­´æ›¸', 'æˆé•·',
                'career', 'job change', 'skill', 'salary', 'interview', 'resume',
                'ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹', 'ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯', 'å‰¯æ¥­', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è»¢è·',
                'ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—', 'ã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒƒãƒ—', 'åƒãæ–¹', 'ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹'
            ],
            'ãƒ“ã‚¸ãƒã‚¹': [
                'ãƒ“ã‚¸ãƒã‚¹', 'æˆ¦ç•¥', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'å£²ä¸Š', 'æˆé•·', 'ROI', 'KPI',
                'business', 'strategy', 'marketing', 'sales', 'growth',
                'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—', 'DX', 'ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©', 'ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³',
                'çµŒå–¶', 'äº‹æ¥­', 'èµ·æ¥­', 'IPO', 'M&A', 'æŠ•è³‡'
            ],
            'å‹‰å¼·ãƒ»è‡ªå·±å•“ç™º': [
                'å­¦ç¿’', 'å‹‰å¼·', 'ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—', 'æˆé•·', 'ç¿’æ…£', 'åŠ¹ç‡', 'ç¶™ç¶š',
                'learning', 'study', 'skill up', 'growth', 'habit', 'efficiency',
                'èª­æ›¸', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’', 'MOOC', 'Udemy', 'Coursera'
            ]
        }
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'Mozilla/5.0 (compatible; TrendBot/1.0)',
                'Accept': 'application/json'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def get_all_trends(self) -> List[TrendItem]:
        """ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ï¼ˆNode.jsç‰ˆã®getAllTrendsç›¸å½“ï¼‰"""
        print("ğŸŒ çµ±åˆãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—é–‹å§‹")
        all_trends = []
        
        try:
            # ä¸¦è¡Œå–å¾—ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
            tasks = [
                self.get_zenn_trending(),
                self.get_hacker_news_trending(),
                self.get_web_dev_trends(),
                self.get_business_trends(),
                self.get_programming_trends(),
                self.get_data_science_trends(),
                self.get_ai_tech_trends()
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, list):
                    all_trends.extend(result)
                elif isinstance(result, Exception):
                    print(f"âš ï¸ ä¸€éƒ¨å–å¾—ã‚¨ãƒ©ãƒ¼: {result}")
            
            # é‡è¤‡é™¤å»
            unique_trends = self._remove_duplicates(all_trends)
            
            print(f"ğŸ“Š çµ±åˆãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—å®Œäº†: {len(unique_trends)}ä»¶ï¼ˆé‡è¤‡é™¤å»å‰: {len(all_trends)}ä»¶ï¼‰")
            return unique_trends
            
        except Exception as e:
            print(f"âŒ çµ±åˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def get_zenn_trending(self) -> List[TrendItem]:
        """Zenn API - æœ€å„ªç§€ã‚½ãƒ¼ã‚¹ï¼ˆ80ä»¶å–å¾—ã€é«˜å“è³ªè¨˜äº‹20%ï¼‰"""
        results = []
        
        try:
            endpoints = [
                'https://zenn.dev/api/articles?order=liked_count&count=100',
                'https://zenn.dev/api/articles?order=trending&count=50',
                'https://zenn.dev/api/articles?order=latest&count=30'
            ]
            
            for endpoint in endpoints:
                try:
                    print(f"ğŸ“š Zenn APIå–å¾—ä¸­: {endpoint}")
                    
                    async with self.session.get(endpoint) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            if data.get('articles'):
                                for article in data['articles']:
                                    # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ã‚’ç·©å’Œ: ã„ã„ã­20+ã¾ãŸã¯ã„ã„ã­10+ AND ã‚³ãƒ¡ãƒ³ãƒˆ3+
                                    liked_count = article.get('liked_count', 0)
                                    comments_count = article.get('comments_count', 0)
                                    
                                    is_high_quality = (liked_count > 20 or 
                                                     (liked_count > 10 and comments_count > 3))
                                    
                                    if is_high_quality:
                                        trend_item = TrendItem(
                                            id=f"zenn-{article.get('id')}",
                                            title=article.get('title', ''),
                                            url=f"https://zenn.dev{article.get('path', '')}",
                                            likes=liked_count,
                                            comments=comments_count,
                                            source='Zenn API',
                                            published_at=article.get('published_at', ''),
                                            topics=[t.get('name', '') for t in article.get('topics', [])]
                                        )
                                        results.append(trend_item)
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: 500mså¾…æ©Ÿ
                    await asyncio.sleep(0.5)
                    
                except Exception as endpoint_error:
                    print(f"âŒ Zenn API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {endpoint} - {endpoint_error}")
            
            print(f"âœ… Zenn APIå–å¾—å®Œäº†: {len(results)}ä»¶")
            return results
            
        except Exception as error:
            print(f"âŒ Zenn APIå–å¾—ã‚¨ãƒ©ãƒ¼: {error}")
            return []
    
    async def get_hacker_news_trending(self) -> List[TrendItem]:
        """Hacker News API - é«˜å“è³ªï¼ˆ30ä»¶å–å¾—ã€æµ·å¤–æŠ€è¡“æƒ…å ±ï¼‰"""
        results = []
        
        try:
            print("ğŸŒ Hacker News APIå–å¾—é–‹å§‹")
            
            # Step 1: ãƒˆãƒƒãƒ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼IDã‚’å–å¾—
            top_stories_url = 'https://hacker-news.firebaseio.com/v0/topstories.json'
            async with self.session.get(top_stories_url) as response:
                if response.status != 200:
                    raise Exception(f"Top storieså–å¾—å¤±æ•—: {response.status}")
                
                top_stories = await response.json()
                if not isinstance(top_stories, list):
                    raise Exception('Invalid top stories format')
            
            # Step 2: ä¸Šä½30ä»¶ã®å€‹åˆ¥è¨˜äº‹ã‚’å–å¾—
            for story_id in top_stories[:30]:
                try:
                    item_url = f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json'
                    async with self.session.get(item_url) as item_response:
                        if item_response.status == 200:
                            item = await item_response.json()
                            
                            # å“è³ªãƒ•ã‚£ãƒ«ã‚¿: ã‚¹ã‚³ã‚¢50+ã€ã‚¿ã‚¤ãƒˆãƒ«ã‚ã‚Šã€URLã‚ã‚Š
                            if (item.get('type') == 'story' and 
                                item.get('score', 0) > 50 and
                                item.get('title') and 
                                item.get('url')):
                                
                                trend_item = TrendItem(
                                    id=f"hn-{story_id}",
                                    title=item.get('title', ''),
                                    url=item.get('url', ''),
                                    score=item.get('score', 0),
                                    comments=item.get('descendants', 0),
                                    source='Hacker News',
                                    published_at=datetime.fromtimestamp(
                                        item.get('time', 0)
                                    ).isoformat() + 'Z'
                                )
                                results.append(trend_item)
                    
                    # APIåˆ¶é™å¯¾ç­–
                    await asyncio.sleep(0.1)
                    
                except Exception as item_error:
                    print(f"âš ï¸ HN itemå–å¾—ã‚¨ãƒ©ãƒ¼: {story_id} - {item_error}")
                    continue
            
            print(f"âœ… Hacker Newså–å¾—å®Œäº†: {len(results)}ä»¶")
            return results
            
        except Exception as error:
            print(f"âŒ Hacker Newså–å¾—ã‚¨ãƒ©ãƒ¼: {error}")
            return []
    
    async def get_web_dev_trends(self) -> List[TrendItem]:
        """ã‚¦ã‚§ãƒ–é–‹ç™ºè¨˜äº‹å–å¾—é–‹å§‹"""
        print("ğŸŒ ã‚¦ã‚§ãƒ–é–‹ç™ºè¨˜äº‹å–å¾—é–‹å§‹")
        results = []
        
        try:
            # Zennè¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            zenn_web_trends = await self._get_zenn_filtered_articles('web development')
            results.extend(zenn_web_trends)
            
            # Google Newsæ¤œç´¢
            web_keywords = ['React é–‹ç™º', 'Vue.js ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰', 'JavaScript ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯', 
                           'ã‚¦ã‚§ãƒ–é–‹ç™º æœ€æ–°', 'CSS ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯']
            
            for keyword in web_keywords:
                google_trends = await self._get_google_news_by_keyword(keyword, 'ã‚¦ã‚§ãƒ–é–‹ç™º')
                results.extend(google_trends)
                await asyncio.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            
            print(f"âœ… ã‚¦ã‚§ãƒ–é–‹ç™ºè¨˜äº‹å–å¾—å®Œäº†: {len(results)}ä»¶")
            return results
            
        except Exception as error:
            print(f"âŒ ã‚¦ã‚§ãƒ–é–‹ç™ºè¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {error}")
            return []
    
    async def get_business_trends(self) -> List[TrendItem]:
        """ãƒ“ã‚¸ãƒã‚¹è¨˜äº‹å°‚é–€å–å¾—"""
        print("ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹è¨˜äº‹å°‚é–€å–å¾—é–‹å§‹")
        results = []
        
        try:
            business_keywords = [
                'DXæ¨é€²', 'ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©', 'ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥',
                'åƒãæ–¹æ”¹é©', 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—', 'äº‹æ¥­æˆé•·', 'ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³'
            ]
            
            for keyword in business_keywords:
                print(f"ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹è¨˜äº‹æ¤œç´¢ä¸­: {keyword}")
                google_trends = await self._get_google_news_by_keyword(keyword, 'ãƒ“ã‚¸ãƒã‚¹')
                results.extend(google_trends)
                await asyncio.sleep(1)
            
            print(f"âœ… ãƒ“ã‚¸ãƒã‚¹è¨˜äº‹å–å¾—å®Œäº†: {len(results)}ä»¶")
            return results
            
        except Exception as error:
            print(f"âŒ ãƒ“ã‚¸ãƒã‚¹è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {error}")
            return []
    
    async def get_programming_trends(self) -> List[TrendItem]:
        """ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨˜äº‹å°‚é–€å–å¾—"""
        print("ğŸ’» ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨˜äº‹å°‚é–€å–å¾—é–‹å§‹")
        results = []
        
        try:
            programming_keywords = [
                'Python ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'Java é–‹ç™º', 'Goè¨€èª', 'Rust ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°',
                'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  å®Ÿè£…', 'ãƒ‡ãƒ¼ã‚¿æ§‹é€ ', 'è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¯ãƒªãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰'
            ]
            
            for keyword in programming_keywords:
                print(f"ğŸ’» ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨˜äº‹æ¤œç´¢ä¸­: {keyword}")
                google_trends = await self._get_google_news_by_keyword(keyword, 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°')
                results.extend(google_trends)
                await asyncio.sleep(1)
            
            print(f"âœ… ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨˜äº‹å–å¾—å®Œäº†: {len(results)}ä»¶")
            return results
            
        except Exception as error:
            print(f"âŒ ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {error}")
            return []
    
    async def get_data_science_trends(self) -> List[TrendItem]:
        """ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»AIé–‹ç™ºè¨˜äº‹å°‚é–€å–å¾—"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»AIé–‹ç™ºè¨˜äº‹å°‚é–€å–å¾—é–‹å§‹")
        results = []
        
        try:
            ds_keywords = [
                'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹', 'æ©Ÿæ¢°å­¦ç¿’ å®Ÿè£…', 'Python ãƒ‡ãƒ¼ã‚¿åˆ†æ', 'pandas æ´»ç”¨',
                'çµ±è¨ˆåˆ†æ', 'ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿', 'ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–', 'MLOps'
            ]
            
            for keyword in ds_keywords:
                print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹è¨˜äº‹æ¤œç´¢ä¸­: {keyword}")
                google_trends = await self._get_google_news_by_keyword(keyword, 'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹')
                results.extend(google_trends)
                await asyncio.sleep(1)
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹è¨˜äº‹å–å¾—å®Œäº†: {len(results)}ä»¶")
            return results
            
        except Exception as error:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {error}")
            return []
    
    async def get_ai_tech_trends(self) -> List[TrendItem]:
        """ç”ŸæˆAIå°‚é–€æƒ…å ±å–å¾—"""
        print("ğŸ¤– ç”ŸæˆAIå°‚é–€æƒ…å ±å–å¾—é–‹å§‹")
        results = []
        
        try:
            ai_keywords = ['claude code', 'chatgpt', 'gemini', 'anthropic', 'openai']
            
            for keyword in ai_keywords:
                google_trends = await self._get_google_news_by_keyword(keyword, 'ç”ŸæˆAI')
                results.extend(google_trends)
                await asyncio.sleep(1)
            
            print(f"âœ… ç”ŸæˆAIå°‚é–€æƒ…å ±: {len(results)}ä»¶å–å¾—")
            return results
            
        except Exception as error:
            print(f"âŒ ç”ŸæˆAIå–å¾—ã‚¨ãƒ©ãƒ¼: {error}")
            return []
    
    async def _get_zenn_filtered_articles(self, filter_keyword: str) -> List[TrendItem]:
        """Zennã‹ã‚‰ç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        results = []
        
        try:
            endpoint = 'https://zenn.dev/api/articles?order=liked_count&count=100'
            async with self.session.get(endpoint) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if data.get('articles'):
                        for article in data['articles']:
                            title = article.get('title', '').lower()
                            if any(kw in title for kw in ['react', 'vue', 'javascript', 'web', 'frontend']):
                                if article.get('liked_count', 0) > 10:
                                    trend_item = TrendItem(
                                        id=f"zenn-web-{article.get('id')}",
                                        title=article.get('title', ''),
                                        url=f"https://zenn.dev{article.get('path', '')}",
                                        likes=article.get('liked_count', 0),
                                        comments=article.get('comments_count', 0),
                                        source='Zenn API',
                                        published_at=article.get('published_at', ''),
                                        topics=[t.get('name', '') for t in article.get('topics', [])]
                                    )
                                    results.append(trend_item)
            
            print(f"ğŸ“š Zennè¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(results)}ä»¶ã®{filter_keyword}è¨˜äº‹ã‚’æŠ½å‡º")
            
        except Exception as error:
            print(f"âŒ Zennãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {error}")
        
        return results
    
    async def _get_google_news_by_keyword(self, keyword: str, category: str) -> List[TrendItem]:
        """Google News RSSæ¤œç´¢"""
        results = []
        
        try:
            print(f"ğŸ” Google Newsæ¤œç´¢ä¸­: {keyword}")
            rss_url = f"https://news.google.com/rss/search?q={quote(keyword)}&hl=ja&gl=JP&ceid=JP:ja"
            
            async with self.session.get(rss_url) as response:
                if response.status == 200:
                    rss_text = await response.text()
                    items = self._parse_rss_feed(rss_text)
                    
                    for i, item in enumerate(items[:5]):  # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰5ä»¶ã¾ã§
                        trend_item = TrendItem(
                            id=f"google-{category.lower()}-{hash(keyword + str(i))}",
                            title=item.get('title', ''),
                            url=item.get('link', ''),
                            source=f'Google News ({category})',
                            published_at=item.get('pubDate', ''),
                            topics=[keyword],
                            description=self._clean_description(item.get('title', ''))
                        )
                        results.append(trend_item)
        
        except Exception as error:
            print(f"âŒ Google Newsæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({keyword}): {error}")
        
        return results
    
    def _parse_rss_feed(self, rss_text: str) -> List[Dict]:
        """RSS XMLãƒ‘ãƒ¼ã‚¹"""
        items = []
        
        try:
            root = ET.fromstring(rss_text)
            for item in root.findall('.//item'):
                title_elem = item.find('title')
                link_elem = item.find('link')
                pub_date_elem = item.find('pubDate')
                
                if title_elem is not None:
                    items.append({
                        'title': title_elem.text or '',
                        'link': link_elem.text if link_elem is not None else '',
                        'pubDate': pub_date_elem.text if pub_date_elem is not None else ''
                    })
        
        except ET.ParseError as e:
            print(f"âŒ RSSè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return items
    
    def _clean_description(self, text: str) -> str:
        """èª¬æ˜æ–‡ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # ã‚µã‚¤ãƒˆåéƒ¨åˆ†ã‚’é™¤å»
        clean_text = re.sub(r'\s*-\s*[^-]*$', '', text)
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(clean_text) > 100:
            clean_text = clean_text[:97] + "..."
        
        return clean_text
    
    def _remove_duplicates(self, trends: List[TrendItem]) -> List[TrendItem]:
        """é‡è¤‡é™¤å»ï¼ˆURLã€ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰"""
        unique_trends = []
        seen_urls = set()
        seen_titles = set()
        
        for trend in trends:
            # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
            if trend.url and trend.url in seen_urls:
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            title_lower = trend.title.lower()
            title_words = set(title_lower.split())
            
            is_similar = False
            for seen_title in seen_titles:
                seen_words = set(seen_title.split())
                # å˜èªã®ä¸€è‡´ç‡ãŒ80%ä»¥ä¸Šãªã‚‰é¡ä¼¼ã¨ã¿ãªã™
                if len(title_words & seen_words) / max(len(title_words), len(seen_words)) > 0.8:
                    is_similar = True
                    break
            
            if not is_similar:
                unique_trends.append(trend)
                if trend.url:
                    seen_urls.add(trend.url)
                seen_titles.add(title_lower)
        
        return unique_trends
    
    def categorize_trends(self, trends: List[TrendItem]) -> Dict[str, List[TrendItem]]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡"""
        categorized = {category: [] for category in self.category_keywords.keys()}
        categorized['é€±é–“ç·åˆ'] = []
        
        for trend in trends:
            title_text = trend.title.lower()
            description_text = trend.description.lower()
            combined_text = f"{title_text} {description_text}"
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
            best_category = 'é€±é–“ç·åˆ'
            max_matches = 0
            
            for category, keywords in self.category_keywords.items():
                matches = sum(1 for keyword in keywords if keyword.lower() in combined_text)
                if matches > max_matches:
                    max_matches = matches
                    best_category = category
            
            trend.category = best_category
            categorized[best_category].append(trend)
        
        return categorized

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
async def demo_realtime_trends():
    """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    async with RealtimeTrendsClient() as client:
        print("ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        
        # å…¨ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—
        all_trends = await client.get_all_trends()
        
        print(f"\nğŸ“Š å–å¾—çµæœ: {len(all_trends)}ä»¶")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡
        categorized = client.categorize_trends(all_trends)
        
        print("\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
        for category, items in categorized.items():
            if items:
                print(f"  {category}: {len(items)}ä»¶")
        
        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        print("\nğŸ”¥ ãƒˆãƒ¬ãƒ³ãƒ‰ã‚µãƒ³ãƒ—ãƒ«:")
        for i, trend in enumerate(all_trends[:5], 1):
            print(f"{i}. [{trend.source}] {trend.title}")
            print(f"   ã‚«ãƒ†ã‚´ãƒª: {trend.category}")
            print(f"   ã‚¹ã‚³ã‚¢: {trend.score}, ã„ã„ã­: {trend.likes}")
            print(f"   URL: {trend.url[:50]}...")
            print()

if __name__ == "__main__":
    asyncio.run(demo_realtime_trends())